{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916d9412-188f-4136-9a45-26ce9c33d931",
   "metadata": {},
   "source": [
    "Notwenige Importe und Konfigurationsfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcadf346-04f4-479e-8b3b-140ff8a858e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import collections\n",
    "sys.path.append(\"src\")  # Falls noch nicht gesetzt\n",
    "\n",
    "from audio_data import AudioDataSet\n",
    "from feature_set import FeatureSet\n",
    "from drone_classifier import DroneClassifier\n",
    "\n",
    "config = {\n",
    "    \"sample_rate\": 16000,\n",
    "    \"audio_length\": 1, # in Sekunden\n",
    "    \"train_path\": \"d:/Dropbox/03 H2 Think/AuDroK mFund/Auswertungen/Datensätze/Drone vs. No Drone/TRAINING/\",\n",
    "    \"val_path\":   \"d:/Dropbox/03 H2 Think/AuDroK mFund/Auswertungen/Datensätze/Drone vs. No Drone/VALIDATION/\",\n",
    "    \"model_file\": \"models/classifier_001.keras\",\n",
    "    \"feature_files\": {\n",
    "        \"train_features\": \"models/train_features.pkl\",\n",
    "        \"val_features\": \"models/val_features.pkl\",\n",
    "        \"train_labels\": \"models/train_labels.pkl\",\n",
    "        \"val_labels\": \"models/val_labels.pkl\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bb7607-a44c-4390-8112-77a68a4ac6ae",
   "metadata": {},
   "source": [
    "Rohdaten fürs Training laden\n",
    "\n",
    "Pre-Emphasis anwenden\n",
    "\n",
    "chunken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a2489b8-6491-46df-9506-4e45fa202ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio files (3266 files to process):\n",
      "[#######################################.] 99.48%\n",
      "39171 Chunks\n",
      "Train class dist: Counter({'drone': 35028, 'no drone': 4143})\n",
      "Counter({'drone': 35028, 'no drone': 4143})\n"
     ]
    }
   ],
   "source": [
    "# Laden des Audiodatensatzes für die Trainingsdaten\n",
    "raw_data_train = AudioDataSet.from_path(\n",
    "    path=config[\"train_path\"],\n",
    "    sample_rate=config[\"sample_rate\"]\n",
    ")\n",
    "\n",
    "# Apply Pre-Emphasis auf die Trainingsrohdaten\n",
    "for audio in raw_data_train.audio:\n",
    "    audio.apply_pre_emphasis()\n",
    "\n",
    "# Chunken der Trainingsrohdaten\n",
    "chunk_length = config[\"sample_rate\"] * config[\"audio_length\"]\n",
    "chunked_data_train = raw_data_train.chunk_all(chunk_length)\n",
    "print(f\"{len(chunked_data_train.audio)} Chunks\")\n",
    "\n",
    "from collections import Counter\n",
    "import random, copy\n",
    "from audio_data import AudioDataSet\n",
    "\n",
    "# Klassen zählen\n",
    "print(\"Train class dist:\", Counter([a.label for a in chunked_data_train.audio]))\n",
    "\n",
    "# Split nach Label\n",
    "dr = [a for a in chunked_data_train.audio if a.label == \"drone\"]\n",
    "nd = [a for a in chunked_data_train.audio if a.label == \"no drone\"]\n",
    "\n",
    "# Anzahl angleichen (Oversampling von no drone)\n",
    "needed = max(0, len(dr) - len(nd))\n",
    "nd_extra = [copy.deepcopy(random.choice(nd)) for _ in range(needed)]\n",
    "\n",
    "# Augmentieren, damit es nicht 1:1 Duplikate sind\n",
    "from augmentations import apply_ground_reflection_to_dataset\n",
    "ranges = {\"src_x\": (-10, 10), \"src_y\": (-10, 10), \"src_z\": (1, 5),\n",
    "          \"mic_z\": (1, 2), \"attenuation\": (0.05, 0.95)}\n",
    "nd_extra_ds = apply_ground_reflection_to_dataset(AudioDataSet(nd_extra),\n",
    "                                                 sample_rate=config[\"sample_rate\"],\n",
    "                                                 ranges=ranges)\n",
    "\n",
    "# Neues balanciertes Trainingsset\n",
    "balanced_train_data = AudioDataSet(dr + nd + nd_extra_ds.audio)\n",
    "\n",
    "# Zwischenspeichern der Trainingsdaten zur Vereinfachung des Debuggings\n",
    "import pickle\n",
    "\n",
    "with open(\"chunked_data_train.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunked_data_train, f)\n",
    "\n",
    "import collections\n",
    "print(collections.Counter([a.label for a in chunked_data_train.audio]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89c31da-3efa-44fe-ae44-608a458c6c2b",
   "metadata": {},
   "source": [
    "Laden der zwischengespeicherten Gechunkten und vorverarbeiteten Trainingsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49d74a74-0534-487c-83ab-bdcb52528fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Laden\n",
    "#import pickle\n",
    "#\n",
    "#with open(\"chunked_data_train.pkl\", \"rb\") as f:\n",
    "#    chunked_data_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6725dbce-6712-4f26-bcdb-cb8a9158499b",
   "metadata": {},
   "source": [
    "Rohdaten für die Validierung laden\n",
    "\n",
    "Pre-Emphasis anwenden\n",
    "\n",
    "chunken\n",
    "\n",
    "Zwischenspeichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e3ad580-aed3-46ae-a8b1-993216db5d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio files (277 files to process):\n",
      "[####################################....] 92.42%\n",
      "34211 Chunks\n"
     ]
    }
   ],
   "source": [
    "raw_data_val = AudioDataSet.from_path(\n",
    "    path=config[\"val_path\"],\n",
    "    sample_rate=config[\"sample_rate\"]\n",
    ")\n",
    "\n",
    "# Pre-Emphasis\n",
    "for audio in raw_data_val.audio:\n",
    "    audio.apply_pre_emphasis()\n",
    "\n",
    "# Chunken\n",
    "chunk_length = config[\"sample_rate\"] * config[\"audio_length\"]\n",
    "chunked_data_val = raw_data_val.chunk_all(chunk_length)\n",
    "print(f\"{len(chunked_data_val.audio)} Chunks\")\n",
    "\n",
    "# Zwischenspeichern der vorverarbeiteten Validierungsdaten\n",
    "import pickle\n",
    "\n",
    "with open(\"chunked_data_val.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunked_data_val, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76478ad1-19c8-4b47-b674-1cc33e7f4f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: Counter({'drone': 35028, 'no drone': 4143})\n",
      "Val  : Counter({'drone': 28575, 'no drone': 5636})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Train:\", Counter([a.label for a in chunked_data_train.audio]))\n",
    "print(\"Val  :\", Counter([a.label for a in chunked_data_val.audio]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2375c9d5-7c1b-448f-bee5-cdbbfcff5005",
   "metadata": {},
   "source": [
    "Laden der zwischengespeicherten vorverarbeiteten Validierungsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb9408bb-dde0-47ea-a5c1-10333c5d7d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Laden\n",
    "#import pickle\n",
    "#\n",
    "#with open(\"chunked_data_val.pkl\", \"rb\") as f:\n",
    "#    chunked_data_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fd688e-dbe2-480a-8c5d-5ff2caf34e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5cb660-efb8-4c5d-8331-f07178a6820f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46e8f47a-e196-4898-856e-6c672073e820",
   "metadata": {},
   "source": [
    "Augmentieren der TRainingsdaten durch das Hinzufügen von Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "955b7c04-f8c4-4f02-a503-33dd969a2543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding Background Noise: 100%|███████████████████████████████████████████████████| 35028/35028 [00:11<00:00, 3122.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Noise-Augmentierung im Dataset-Stil (ohne X_/y_-Vorgriffe) ---\n",
    "\n",
    "from augmentations import apply_noise_augmentation\n",
    "from audio_data import AudioDataSet\n",
    "\n",
    "# 1) Noise-Dataset aus echten No-Drone-Validierungs-Chunks bauen\n",
    "noise_from_val = [a for a in chunked_data_val.audio if a.label.lower() in (\"no drone\", \"no_drone\", \"no-drone\")]\n",
    "noise_ds = AudioDataSet(noise_from_val)\n",
    "\n",
    "# 2) Drohnen im Trainingsset separieren (balanced_train_data existiert bei dir bereits)\n",
    "drone_train = [a for a in balanced_train_data.audio if a.label.lower() == \"drone\"]\n",
    "no_drone_train = [a for a in balanced_train_data.audio if a.label.lower() != \"drone\"]\n",
    "\n",
    "drone_train_ds = AudioDataSet(drone_train)\n",
    "no_drone_train_ds = AudioDataSet(no_drone_train)\n",
    "\n",
    "# 3) Drohnen mit Val-Noise mischen (SNR-Logik steckt in apply_noise_augmentation)\n",
    "#    max_snr_db belassen oder moderat setzen. Höher = weniger Noise, niedriger/negativ = mehr Noise.\n",
    "aug_drone_ds = apply_noise_augmentation(\n",
    "    drone_dataset=drone_train_ds,\n",
    "    noise_dataset=no_drone_train_ds if len(no_drone_train) > 0 else noise_ds,  # Fallback\n",
    "    sample_rate=config[\"sample_rate\"],\n",
    "    # max_snr_db=-6  # optional anpassen, falls du härtere Bedingungen willst\n",
    ")\n",
    "\n",
    "# 4) Neues Trainings-Dataset zusammensetzen: (augmentierte Drohnen) + (No-Drone unverändert)\n",
    "augmented_train_data = AudioDataSet(aug_drone_ds.audio + no_drone_train_ds.audio)\n",
    "\n",
    "# 5) Optional: dezentes Echo oben drauf (deine bestehende Funktion arbeitet ebenfalls auf AudioDataSet)\n",
    "from augmentations import apply_ground_reflection_to_dataset\n",
    "ranges = {\"src_x\": (-10, 10), \"src_y\": (-10, 10), \"src_z\": (1, 5),\n",
    "          \"mic_z\": (1, 2), \"attenuation\": (0.05, 0.95)}\n",
    "augmented_train_data = apply_ground_reflection_to_dataset(\n",
    "    augmented_train_data, sample_rate=config[\"sample_rate\"], ranges=ranges\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33082085-68b2-477c-ab48-bcfe39166a98",
   "metadata": {},
   "source": [
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00c5001b-42d3-4f91-9e51-44fcfdb75025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features (70056 files to process):\n",
      "[########################################] 100.00%\n",
      "Processing features (34211 files to process):\n",
      "[########################################] 100.00%\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m X_val, y_val = val_features.extract()\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Training starten\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m clf = \u001b[43mDroneClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m clf.train(X_train, y_train, X_val, y_val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Dropbox\\03 H2 Think\\AuDroK mFund\\Auswertungen\\25-03 Drone Classifier\\src\\drone_classifier.py:74\u001b[39m, in \u001b[36mDroneClassifier.__init__\u001b[39m\u001b[34m(self, model_path, trainable_layers, scaler_path)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28mself\u001b[39m.history = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler_path = scaler_path \u001b[38;5;129;01mor\u001b[39;00m (\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m] + \u001b[33m\"\u001b[39m\u001b[33m_scaler.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen ntpath>:258\u001b[39m, in \u001b[36msplitext\u001b[39m\u001b[34m(p)\u001b[39m\n",
      "\u001b[31mTypeError\u001b[39m: expected str, bytes or os.PathLike object, not dict"
     ]
    }
   ],
   "source": [
    "# Feature-Extraktion (liefert X_*, y_*)\n",
    "train_features = FeatureSet(augmented_train_data, sample_rate=config[\"sample_rate\"])\n",
    "X_train, y_train = train_features.extract()\n",
    "\n",
    "val_features = FeatureSet(chunked_data_val, sample_rate=config[\"sample_rate\"])\n",
    "X_val, y_val = val_features.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2376b312-e5dc-4158-a35f-9a0cc3d01f84",
   "metadata": {},
   "source": [
    "Zwischenspeichern der Features zur Vereinfachung des Debuggings (funktioniert derzeit nicht korrekt, beim Laden ändert sich das Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4eab59af-7161-4220-9f71-b25c7ce25b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Speichern\n",
    "#import pickle\n",
    "#\n",
    "#with open(\"features.pkl\", \"wb\") as f:\n",
    "#    pickle.dump({\n",
    "#        \"X_train\": X_train,\n",
    "#        \"y_train\": y_train,\n",
    "#        \"X_val\": X_val,\n",
    "#        \"y_val\": y_val\n",
    "#    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a709b3c-ba73-40cc-860f-386aa4b738eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Laden\n",
    "#import pickle\n",
    "#\n",
    "#with open(\"features.pkl\", \"rb\") as f:\n",
    "#    data = pickle.load(f)\n",
    "\n",
    "#X_train = data[\"X_train\"]\n",
    "#y_train = data[\"y_train\"]\n",
    "#X_val = data[\"X_val\"]\n",
    "#y_val = data[\"y_val\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590ed313-2112-4489-9a8a-783971bf0289",
   "metadata": {},
   "source": [
    "Modelltraining & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e930fa3-4988-4719-9152-0f3990231696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {np.str_('drone'): np.int64(0), np.str_('no drone'): np.int64(1)}\n",
      "Encoded Labels - Train: [0 1]\n",
      "Encoded Labels - Validation: [0 1]\n",
      "Class weights: {0: np.float64(1.0), 1: np.float64(1.0)}\n",
      "Epoch 1/30\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2271s\u001b[0m 2s/step - auc: 0.9863 - auprc: 0.9840 - loss: 0.1671 - precision: 0.9413 - recall: 0.9506 - val_auc: 0.7811 - val_auprc: 0.4039 - val_loss: 1.7438 - val_precision: 0.2297 - val_recall: 0.9079 - learning_rate: 5.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m914s\u001b[0m 835ms/step - auc: 0.9944 - auprc: 0.9936 - loss: 0.1091 - precision: 0.9639 - recall: 0.9689 - val_auc: 0.5936 - val_auprc: 0.2002 - val_loss: 1.7536 - val_precision: 0.2008 - val_recall: 0.7445 - learning_rate: 5.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m633s\u001b[0m 579ms/step - auc: 0.9960 - auprc: 0.9951 - loss: 0.0891 - precision: 0.9714 - recall: 0.9752 - val_auc: 0.7008 - val_auprc: 0.2904 - val_loss: 2.2108 - val_precision: 0.2120 - val_recall: 0.8614 - learning_rate: 5.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 559ms/step - auc: 0.9974 - auprc: 0.9969 - loss: 0.0723 - precision: 0.9767 - recall: 0.9797 - val_auc: 0.6525 - val_auprc: 0.2326 - val_loss: 2.7638 - val_precision: 0.2094 - val_recall: 0.8770 - learning_rate: 5.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m583s\u001b[0m 532ms/step - auc: 0.9980 - auprc: 0.9977 - loss: 0.0615 - precision: 0.9797 - recall: 0.9828 - val_auc: 0.7475 - val_auprc: 0.3402 - val_loss: 1.9160 - val_precision: 0.2316 - val_recall: 0.8641 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m576s\u001b[0m 526ms/step - auc: 0.9994 - auprc: 0.9993 - loss: 0.0366 - precision: 0.9892 - recall: 0.9915 - val_auc: 0.6934 - val_auprc: 0.2843 - val_loss: 1.9946 - val_precision: 0.2413 - val_recall: 0.7560 - learning_rate: 2.5000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m576s\u001b[0m 526ms/step - auc: 0.9997 - auprc: 0.9996 - loss: 0.0284 - precision: 0.9924 - recall: 0.9935 - val_auc: 0.7208 - val_auprc: 0.2918 - val_loss: 2.6683 - val_precision: 0.2356 - val_recall: 0.8400 - learning_rate: 2.5000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m572s\u001b[0m 522ms/step - auc: 0.9998 - auprc: 0.9997 - loss: 0.0230 - precision: 0.9939 - recall: 0.9949 - val_auc: 0.6896 - val_auprc: 0.2646 - val_loss: 2.8151 - val_precision: 0.2395 - val_recall: 0.7837 - learning_rate: 2.5000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m1095/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m569s\u001b[0m 520ms/step - auc: 0.9997 - auprc: 0.9996 - loss: 0.0220 - precision: 0.9939 - recall: 0.9947 - val_auc: 0.6864 - val_auprc: 0.2556 - val_loss: 4.0390 - val_precision: 0.2166 - val_recall: 0.8691 - learning_rate: 2.5000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m1016/1095\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m32s\u001b[0m 415ms/step - auc: 0.9999 - auprc: 0.9999 - loss: 0.0132 - precision: 0.9977 - recall: 0.9978"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = DroneClassifier(model_path=\"models/classifier_010.keras\", trainable_layers=5)\n",
    "model.train(X_train, y_train, X_val, y_val)\n",
    "accuracy = model.evaluate(X_val, y_val)\n",
    "\n",
    "print(f\"Validierungsgenauigkeit: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd684537-2de1-4652-9b9b-4844d1916a43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
