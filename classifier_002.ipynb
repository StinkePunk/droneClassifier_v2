{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916d9412-188f-4136-9a45-26ce9c33d931",
   "metadata": {},
   "source": [
    "Notwenige Importe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcadf346-04f4-479e-8b3b-140ff8a858e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"src\")  # Falls noch nicht gesetzt\n",
    "\n",
    "from audio_data import AudioDataSet\n",
    "from feature_set import FeatureSet\n",
    "from classifier import DroneClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5533e2-d394-4f03-8f04-537baea744bd",
   "metadata": {},
   "source": [
    "Konfigurationsfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35dc1f58-5ba0-4106-a295-f9ad12f87d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"sample_rate\": 16000,\n",
    "    \"audio_length\": 1, # in Sekunden\n",
    "    \"train_path\": \"d:/Dropbox/03 H2 Think/AuDroK mFund/Auswertungen/Datensätze/Drone vs. No Drone/TRAINING/\",\n",
    "    \"val_path\":   \"d:/Dropbox/03 H2 Think/AuDroK mFund/Auswertungen/Datensätze/Drone vs. No Drone/VALIDATION/\",\n",
    "    \"model_file\": \"models/classifier_001.keras\",\n",
    "    \"feature_files\": {\n",
    "        \"train_features\": \"models/train_features.pkl\",\n",
    "        \"val_features\": \"models/val_features.pkl\",\n",
    "        \"train_labels\": \"models/train_labels.pkl\",\n",
    "        \"val_labels\": \"models/val_labels.pkl\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bb7607-a44c-4390-8112-77a68a4ac6ae",
   "metadata": {},
   "source": [
    "Rohdaten fürs Training laden und inspizieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a82e8d5-01a9-4fe2-b1f2-6947f4cb1a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio files (3267 files to process):\n",
      "[#######################################.] 99.45%\n"
     ]
    }
   ],
   "source": [
    "raw_data = AudioDataSet.from_path(\n",
    "    path=config[\"train_path\"],\n",
    "    sample_rate=config[\"sample_rate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3fbc1c-f93e-45c6-8157-cb6832c002f1",
   "metadata": {},
   "source": [
    "Apply Pre-Emphasis auf die Trainingsrohdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa655e83-5a06-494d-9a65-e7537c15ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio in raw_data.audio:\n",
    "    audio.apply_pre_emphasis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f252ab7a-0293-4fde-a1e1-08d4c45a5dfa",
   "metadata": {},
   "source": [
    "Chunken der Trainingsrohdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "345182d7-a456-420e-a7c6-65c16e626bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39171 Chunks\n"
     ]
    }
   ],
   "source": [
    "chunk_length = config[\"sample_rate\"] * config[\"audio_length\"]\n",
    "chunked_data = raw_data.chunk_all(chunk_length)\n",
    "print(f\"{len(chunked_data.audio)} Chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b34e838-82f0-4c8b-9364-e9813e28d567",
   "metadata": {},
   "source": [
    "Zwischenspeichern der Trainingsdaten zur Vereinfachung des Debuggings (kann später wieder entfernt werden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89a5c11a-e234-495d-8d22-6b4cc22eb31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern\n",
    "import pickle\n",
    "\n",
    "with open(\"chunked_data.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunked_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d74a74-0534-487c-83ab-bdcb52528fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden\n",
    "import pickle\n",
    "\n",
    "with open(\"chunked_data.pkl\", \"rb\") as f:\n",
    "    chunked_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be3fdb1-4afd-4e61-869f-50991ff294c0",
   "metadata": {},
   "source": [
    "Augmentieren der Chunks durch das Hinzufügen von Bodenreflexionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "955b7c04-f8c4-4f02-a503-33dd969a2543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from augmentations import apply_ground_reflection_to_dataset\n",
    "\n",
    "ranges = {\n",
    "    \"src_x\": (-10, 10),\n",
    "    \"src_y\": (-10, 10),\n",
    "    \"src_z\": (1, 5),\n",
    "    \"mic_z\": (1, 2),\n",
    "    \"attenuation\": (0.05, 0.95)\n",
    "}\n",
    "\n",
    "augmented_data = apply_ground_reflection_to_dataset(\n",
    "    dataset=chunked_data,\n",
    "    sample_rate=config[\"sample_rate\"],\n",
    "    ranges=ranges\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6725dbce-6712-4f26-bcdb-cb8a9158499b",
   "metadata": {},
   "source": [
    "Rohdaten für die Validierung laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e3ad580-aed3-46ae-a8b1-993216db5d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing audio files (277 files to process):\n",
      "[####################################....] 92.42%\n"
     ]
    }
   ],
   "source": [
    "raw_data_val = AudioDataSet.from_path(\n",
    "    path=config[\"val_path\"],\n",
    "    sample_rate=config[\"sample_rate\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c798f5-b0fb-4f3a-b133-6a71bf195d1c",
   "metadata": {},
   "source": [
    "Apply Pre-Emphasis auf die Validierungsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7c0e13a-aeb7-4718-abf4-cc885220ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio in raw_data_val.audio:\n",
    "    audio.apply_pre_emphasis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6708bb0e-8838-4f6f-b4af-fac3dc930cfc",
   "metadata": {},
   "source": [
    "Chunken der Validierungsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d0d85c0-e896-40ac-8fb5-f350fe04334c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34211 Chunks\n"
     ]
    }
   ],
   "source": [
    "chunk_length = config[\"sample_rate\"] * config[\"audio_length\"]\n",
    "chunked_data_val = raw_data_val.chunk_all(chunk_length)\n",
    "print(f\"{len(chunked_data_val.audio)} Chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2375c9d5-7c1b-448f-bee5-cdbbfcff5005",
   "metadata": {},
   "source": [
    "Zwischenspeichern der Validierungsdaten zur Vereinfachung des Debuggings (kann später wieder entfernt werden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96258841-a73a-4d69-8616-bb72d169b219",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichern\n",
    "import pickle\n",
    "\n",
    "with open(\"chunked_data_val.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chunked_data_val, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9408bb-dde0-47ea-a5c1-10333c5d7d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden\n",
    "import pickle\n",
    "\n",
    "with open(\"chunked_data_val.pkl\", \"rb\") as f:\n",
    "    chunked_data_val = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33082085-68b2-477c-ab48-bcfe39166a98",
   "metadata": {},
   "source": [
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00c5001b-42d3-4f91-9e51-44fcfdb75025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features (39171 files to process):\n",
      "[########################################] 100.00%\n",
      "Processing features (34211 files to process):\n",
      "[########################################] 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Feature-Extraktion\n",
    "train_features = FeatureSet(augmented_data, sample_rate=config[\"sample_rate\"])\n",
    "X_train, y_train = train_features.extract()\n",
    "\n",
    "val_features = FeatureSet(chunked_data_val, sample_rate=config[\"sample_rate\"])\n",
    "X_val, y_val = val_features.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590ed313-2112-4489-9a8a-783971bf0289",
   "metadata": {},
   "source": [
    "Modelltraining & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e22ed9c-3a0a-4ce1-918a-5292c797c9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Labels - Train: [0 1]\n",
      "Encoded Labels - Validation: [0 1]\n",
      "Class weights: {0: np.float64(0.559138403562864), 1: np.float64(4.727371469949312)}\n",
      "Epoch 1/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m732s\u001b[0m 1s/step - accuracy: 0.8509 - loss: 1.8983 - val_accuracy: 0.4775 - val_loss: 0.8862 - learning_rate: 5.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m662s\u001b[0m 1s/step - accuracy: 0.9882 - loss: 0.0445 - val_accuracy: 0.6862 - val_loss: 0.2355 - learning_rate: 5.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m655s\u001b[0m 1s/step - accuracy: 0.9884 - loss: 0.0617 - val_accuracy: 0.3554 - val_loss: 1.5770 - learning_rate: 5.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m636s\u001b[0m 1s/step - accuracy: 0.9883 - loss: 0.2054 - val_accuracy: 0.5701 - val_loss: 0.3839 - learning_rate: 5.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m645s\u001b[0m 1s/step - accuracy: 0.9923 - loss: 0.0578 - val_accuracy: 0.6022 - val_loss: 0.1819 - learning_rate: 5.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m640s\u001b[0m 1s/step - accuracy: 0.9931 - loss: 0.1370 - val_accuracy: 0.4088 - val_loss: 0.4951 - learning_rate: 5.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m628s\u001b[0m 1s/step - accuracy: 0.9888 - loss: 0.2638 - val_accuracy: 0.6161 - val_loss: 0.1480 - learning_rate: 5.0000e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m630s\u001b[0m 1s/step - accuracy: 0.9925 - loss: 0.1289 - val_accuracy: 0.3541 - val_loss: 0.3290 - learning_rate: 5.0000e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m630s\u001b[0m 1s/step - accuracy: 0.9951 - loss: 0.0389 - val_accuracy: 0.4825 - val_loss: 0.5427 - learning_rate: 5.0000e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 1s/step - accuracy: 0.9945 - loss: 0.0717 - val_accuracy: 0.5112 - val_loss: 0.1484 - learning_rate: 5.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 1s/step - accuracy: 0.9956 - loss: 0.0414 - val_accuracy: 0.5028 - val_loss: 0.2077 - learning_rate: 5.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 1s/step - accuracy: 0.9978 - loss: 0.0157 - val_accuracy: 0.4680 - val_loss: 0.1192 - learning_rate: 2.5000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m616s\u001b[0m 1s/step - accuracy: 0.9980 - loss: 0.0612 - val_accuracy: 0.8125 - val_loss: 0.0467 - learning_rate: 2.5000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m616s\u001b[0m 1s/step - accuracy: 0.9958 - loss: 0.0795 - val_accuracy: 0.4997 - val_loss: 0.1803 - learning_rate: 2.5000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 1s/step - accuracy: 0.9967 - loss: 0.0872 - val_accuracy: 0.4306 - val_loss: 0.3788 - learning_rate: 2.5000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m617s\u001b[0m 1s/step - accuracy: 0.9986 - loss: 0.0187 - val_accuracy: 0.3826 - val_loss: 0.3843 - learning_rate: 2.5000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 1s/step - accuracy: 0.9986 - loss: 0.0137 - val_accuracy: 0.8274 - val_loss: 0.0372 - learning_rate: 2.5000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m616s\u001b[0m 1s/step - accuracy: 0.9978 - loss: 0.0459 - val_accuracy: 0.4052 - val_loss: 0.1586 - learning_rate: 2.5000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 1s/step - accuracy: 0.9982 - loss: 0.0122 - val_accuracy: 0.5783 - val_loss: 0.1199 - learning_rate: 2.5000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m612s\u001b[0m 998ms/step - accuracy: 0.9988 - loss: 0.0098 - val_accuracy: 0.4067 - val_loss: 0.4418 - learning_rate: 2.5000e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m608s\u001b[0m 991ms/step - accuracy: 0.9987 - loss: 0.0454 - val_accuracy: 0.4556 - val_loss: 0.3432 - learning_rate: 2.5000e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 994ms/step - accuracy: 0.9995 - loss: 0.0044 - val_accuracy: 0.8340 - val_loss: 0.0313 - learning_rate: 1.2500e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m610s\u001b[0m 995ms/step - accuracy: 0.9991 - loss: 0.0336 - val_accuracy: 0.4799 - val_loss: 0.2178 - learning_rate: 1.2500e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 1s/step - accuracy: 0.9994 - loss: 0.0063 - val_accuracy: 0.3664 - val_loss: 0.2595 - learning_rate: 1.2500e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m620s\u001b[0m 1s/step - accuracy: 0.9996 - loss: 0.0037 - val_accuracy: 0.4314 - val_loss: 0.7600 - learning_rate: 1.2500e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m608s\u001b[0m 993ms/step - accuracy: 0.9994 - loss: 0.0133 - val_accuracy: 0.4509 - val_loss: 0.2365 - learning_rate: 1.2500e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 1s/step - accuracy: 0.9998 - loss: 0.0019 - val_accuracy: 0.4995 - val_loss: 0.2615 - learning_rate: 6.2500e-05\n",
      "Epoch 28/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m611s\u001b[0m 997ms/step - accuracy: 0.9992 - loss: 0.0067 - val_accuracy: 0.4812 - val_loss: 0.1441 - learning_rate: 6.2500e-05\n",
      "Epoch 29/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m608s\u001b[0m 991ms/step - accuracy: 0.9999 - loss: 0.0018 - val_accuracy: 0.4321 - val_loss: 2.0153 - learning_rate: 6.2500e-05\n",
      "Epoch 30/30\n",
      "\u001b[1m613/613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m616s\u001b[0m 1s/step - accuracy: 0.9998 - loss: 0.0122 - val_accuracy: 0.4163 - val_loss: 0.5704 - learning_rate: 6.2500e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1070/1070\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m151s\u001b[0m 141ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mix of label input types (string and number)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m model = model = DroneClassifier(model_path=\u001b[33m\"\u001b[39m\u001b[33mmodels/classifier_002.h5\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m model.train(X_train, y_train, X_val, y_val)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m accuracy = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValidierungsgenauigkeit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mD:\\Dropbox\\03 H2 Think\\AuDroK mFund\\Auswertungen\\25-03 Drone Classifier\\src\\classifier.py:45\u001b[39m, in \u001b[36mDroneClassifier.evaluate\u001b[39m\u001b[34m(self, X_val, y_val)\u001b[39m\n\u001b[32m     43\u001b[39m y_pred_prob = \u001b[38;5;28mself\u001b[39m.predict(X_val)\n\u001b[32m     44\u001b[39m y_pred = (y_pred_prob > \u001b[32m0.5\u001b[39m).astype(\u001b[38;5;28mint\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m cm = \u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m plot_confusion_matrix(cm)\n\u001b[32m     47\u001b[39m plot_probability_distribution(y_pred_prob)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    212\u001b[39m         skip_parameter_validation=(\n\u001b[32m    213\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    214\u001b[39m         )\n\u001b[32m    215\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    222\u001b[39m     msg = re.sub(\n\u001b[32m    223\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    224\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    225\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    226\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:345\u001b[39m, in \u001b[36mconfusion_matrix\u001b[39m\u001b[34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[39m\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m is not supported\u001b[39m\u001b[33m\"\u001b[39m % y_type)\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m     labels = \u001b[43munique_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    347\u001b[39m     labels = np.asarray(labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\multiclass.py:117\u001b[39m, in \u001b[36munique_labels\u001b[39m\u001b[34m(*ys)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m# Check that we don't mix string type with number type\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(label, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m ys_labels)) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMix of label input types (string and number)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m xp.asarray(\u001b[38;5;28msorted\u001b[39m(ys_labels))\n",
      "\u001b[31mValueError\u001b[39m: Mix of label input types (string and number)"
     ]
    }
   ],
   "source": [
    "model = DroneClassifier(model_path=\"models/classifier_002.h5\")\n",
    "model.train(X_train, y_train, X_val, y_val)\n",
    "accuracy = model.evaluate(X_val, y_val)\n",
    "\n",
    "print(f\"Validierungsgenauigkeit: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e930fa3-4988-4719-9152-0f3990231696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
